# VisualMathAI

<p align="center">
  <a href="https://huggingface.co/spaces/huggingface-projects/agents-mcp-hackathon">
    <img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Agents--MCP%20Hackathon-blue" alt="Hugging Face Agents-MCP Hackathon">
  </a>
</p>

<p align="center">
  <strong>An advanced, interactive web application that combines a conversational AI with dynamic visualization capabilities.</strong>
  <br />
  Ask questions, explore complex concepts, and watch as the AI generates on-the-fly graphs, animations, and interactive learning widgets.
</p>

<p align="center">
  <a href="#-project-context">Project Context</a> •
  <a href="#-features">Features</a> •
  <a href="#-architecture-the-model-context-protocol">Architecture</a> •
  <a href="#-technology-stack">Tech Stack</a> •
  <a href="#-getting-started">Getting Started</a> •
  <a href="#-acknowledgments">Acknowledgments</a>
</p>

---

## 💡 Project Context

This project was initially developed for the **Hugging Face Agents & Model-Context Protocol (MCP) Hackathon**.

The core architecture is a direct implementation of the MCP philosophy: treating the Large Language Model (LLM) not just as a text generator, but as an intelligent **agentic planner**. The LLM's role is to understand user intent and generate a structured `ToolCall`—a formal plan—which the backend system then orchestrates and executes using specialized tools like renderers and sandboxed environments. This approach forms the foundation of the application's advanced capabilities.

## ✨ Features

*   **Conversational AI Interface:** Chat with an expert AI assistant powered by state-of-the-art models from OpenAI or Anthropic.
*   **Agent-Like Orchestration:** The AI acts as a "planner," deciding whether to respond with text or to build a complex, interactive "Learning Widget" to explain a concept, all governed by the MCP.
*   **Composite Visualizations:** Generates multi-asset "Learning Widgets" that combine videos, plots, and interactive controls into a single, cohesive experience.
*   **Multi-Modal Rendering Engine:**
    *   **Manim:** For broadcast-quality mathematical animations.
    *   **Plotly:** For rich, interactive 2D/3D charts.
    *   **Interactive JS/HTML:** For custom-built, controllable diagrams and simulations.
*   **Scalable & Secure Cloud Backend:** Heavy-duty tasks like LLM inference and video rendering are offloaded to **Modal Labs**, ensuring the application is responsive, secure, and can scale on demand.
*   **Persistent Sessions:** Leverages a SQLite backend to save and manage conversation history and the state of generated widgets across sessions.
*   **Sandboxed Execution:** All generated code for rendering is executed in secure, isolated containers on Modal, protecting the system's integrity.

## 🏗️ Architecture: The Model-Context Protocol (MCP)

The project is built on a modern, distributed three-tier architecture orchestrated by the **Model-Context Protocol (MCP)**. This protocol defines the data structures and conventions for state management, making the system robust and extensible.

```
┌──────────────────────────┐      ┌───────────────────────────┐      ┌──────────────────────────────────┐
│  Gradio Frontend Client  │      │   FastAPI Backend Server  │      │     Modal Serverless Backend     │
│  (Renders UI & Widgets)  ├─HTTP─►│  (MCP Orchestrator)       ├─RPC──►│     (MCP Tool Executor)          │
└──────────────────────────┘      └───────────────────────────┘      └──────────────────────────────────┘
```

1.  **Gradio Frontend (`main.py`):** A lightweight client that provides the UI. It makes HTTP API calls to the FastAPI backend and is responsible for rendering the final text and "Learning Widgets."

2.  **FastAPI Backend (`backend/`):** The central application server and **MCP Orchestrator**. It manages session state, but its primary role is to interpret `ToolCall` plans generated by the LLM. It calls the appropriate Modal functions to execute the plan, updates the session context, and assembles the final response for the frontend.

3.  **Modal Serverless Backend (`modal_runners/`):** A set of powerful, secure, serverless functions that act as **MCP Tool Executors**.
    *   **LLM Function (`llm_inference.py`):** The MCP "Planner." Its sole job is to decide which "tool" to use and return a structured `ToolCall` JSON object.
    *   **Widget Executor (`widget_executor.py`):** A sandboxed function that receives asset generation tasks (e.g., "render this Manim scene") and executes them in a secure environment.

### How the Model-Context Protocol Works

The MCP is the core of the system's intelligence. Instead of just generating text, the LLM generates a structured **`ToolCall`** object, which is an explicit instruction for the backend.

**Example Flow:**
1.  **User:** "Show me how sine waves build a square wave."
2.  **FastAPI** sends this to the **Modal LLM Function**.
3.  **Modal LLM** returns a `ToolCall` with `tool_name: "generate_learning_widget"` and a detailed plan. This plan includes the explanation text, a list of Manim scenes to render, and the HTML/JS code for an interactive player.
4.  **FastAPI** receives and validates this plan.
5.  **FastAPI** sends the list of Manim scenes to the **Modal Widget Executor Function**.
6.  **Modal Widget Executor** renders the videos in a secure sandbox and returns the video data.
7.  **FastAPI** saves the videos to its cache, generates URLs, and injects these URLs into the HTML/JS player code from the plan.
8.  **FastAPI** sends the final, fully-formed widget to the **Gradio Frontend**.
9.  **Gradio** renders the interactive widget in an iframe for the user.

## 💻 Technology Stack

| Category            | Technology                                           |
| ------------------- | ---------------------------------------------------- |
| **Frontend**        | Gradio                                               |
| **Backend Server**  | FastAPI, Uvicorn                                     |
| **Cloud Backend**   | Modal Labs                                           |
| **LLM Interaction** | OpenAI, Anthropic SDKs                               |
| **Data Modeling**   | Pydantic                                             |
| **Database**        | SQLite with `aiosqlite`                              |
| **Rendering**       | Manim, Plotly, HTML5 Canvas                          |
| **Containerization**| Docker, Docker Compose                               |
| **HTTP Client**     | `httpx`                                              |

## 📁 Project Structure

```
visual-math-ai/
├── main.py                 # Gradio Frontend Client application
├── requirements.txt        # Dependencies for the Gradio Frontend
│
├── backend/                # FastAPI Backend Server application
│   ├── app/                # Main source code for the backend
│   └── requirements.txt    # Dependencies for the Backend
│
├── modal_runners/          # Code for functions deployed to Modal
│
├── config/                 # Deployment configurations (Docker, Nginx)
│
├── runtime/                # Directory for runtime files (cache, db)
│
├── .env                    # Local environment variables
└── setup.sh                # Automated setup script
```

## 🚀 Getting Started

This project involves three main components that need to be running: the Gradio Frontend, the FastAPI Backend, and the Modal Backend.

### Prerequisites

*   Python 3.9+
*   [Modal CLI](https://modal.com/docs/guide/local-development#installing-the-`modal`-cli) installed and authenticated (`modal token new`).
*   Docker & Docker Compose.
*   An active `.env` file with API keys.

### Local Development Setup

**1. Initial Setup**

Clone the repository and run the setup script to prepare your environment.

```bash
git clone https://github.com/your-username/visual-math-ai.git
cd visual-math-ai
./setup.sh
```
After running, **edit the newly created `.env` file** to add your required API keys (`OPENAI_API_KEY`, `MODAL_TOKEN_ID`, etc.).

**2. Deploy the Modal Backend**

Deploy the serverless functions to Modal's platform with a single command:

```bash
# This command finds all @app.function decorators and deploys them.
modal deploy modal_runners/widget_executor.py
```

**3. Run the Local Services**

We recommend using Docker Compose to run the backend and frontend services, as it handles the networking between them.

```bash
# This will build the containers and start both services.
docker-compose -f config/docker/docker-compose.yml up --build
```

You can now access the Gradio interface at `http://localhost:7860`.

## ⚙️ Configuration

*   **Environment Variables:** All configuration, especially secrets like API keys and Modal tokens, is managed via a `.env` file. Refer to `.env.example` for the full list of required variables.
*   **Modal Secrets:** For production deployments, secrets should be stored directly on the Modal platform for maximum security.

## 🛡️ Security

*   **API Keys:** Handled securely using Modal Secrets, ensuring they are never exposed in the client-facing backend or frontend code.
*   **Sandboxed Rendering:** All asset generation (Manim, etc.) occurs within `modal.Sandbox`, a secure, isolated container environment, mitigating risks from executing LLM-generated code.
*   **Web Security:** Standard practices like CORS are configured in the FastAPI backend to protect the API.

## 🤝 Contributing

We welcome contributions! Whether it's improving the MCP, adding a new renderer, or enhancing the UI, your input is valuable. Please fork the repository, make your changes in a separate branch, and submit a pull request with a clear description of your work.

## 🙏 Acknowledgments

This project stands on the shoulders of giants and was inspired by cutting-edge concepts in the AI and open-source communities.

*   **Anthropic's Claude:** The advanced tool-use and agentic reasoning capabilities of Claude models were a direct inspiration for the **Model-Context Protocol (MCP)** architecture used in this project. The idea of an LLM acting as a "planner" that generates structured instructions for a system of tools is central to our design.

*   **Manim Community:** This project would not be possible without the incredible **Manim** mathematical animation engine. We extend our thanks to Grant Sanderson (3Blue1Brown) for creating it and to the vibrant Manim Community for maintaining and expanding its capabilities.

*   **Modal Labs:** The ability to securely sandbox code and scale resource-intensive tasks like rendering and inference is powered by the serverless infrastructure of **Modal**. Their platform made this complex distributed architecture feasible and efficient.

*   **Hugging Face:** The initial prototype and design philosophy were developed for the **Hugging Face Agents & MCP Hackathon**, which provided the motivation and conceptual framework for this project.

*   **Open Source Ecosystem:** We are deeply grateful to the developers and maintainers of FastAPI, Gradio, Pydantic, Plotly, and the many other open-source libraries that form the backbone of this application.

## 📄 License

This project is licensed under the MIT License - see the `LICENSE.md` file for details.